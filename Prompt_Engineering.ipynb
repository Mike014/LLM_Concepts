{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prompt Engineering**\n",
    "\n",
    "A **LLM** **predicts tokens** one after another **based on context and training**.\n",
    "**Prompt engineering** means creating clear and optimized prompts to guide the model to give correct answers.\n",
    "You need to **experiment** with the structure, style, length and parameters of the model (e.g. temperature, top-k).\n",
    "Prompts allow you to do: **summaries, Q&A, classifications, translations, code generation and more**.\n",
    "Each LLM can require different prompts: it must be **adapted to the model used** (Gemini, GPT, LLaMA, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LLM Output Configuration**\n",
    "\n",
    "Once you have chosen a model, it is essential to **configure it well** to get outputs suitable for your task.\n",
    "\n",
    "#### **Output Length**\n",
    "- Controls **how many tokens** the model can generate.\n",
    "- **More tokens** = more cost, slowness, energy consumption.\n",
    "- Does not make the output more synthetic, only shorter.\n",
    "- Useful to limit it in techniques like **ReAct** to avoid useless outputs.\n",
    "\n",
    "#### **Sampling Controls**\n",
    "LLMs do not choose a fixed token but **calculate a probability distribution**. These settings regulate **how random or creative** the output will be:\n",
    "\n",
    "- **Temperature**\n",
    "- **0** = deterministic (always the most likely token).\n",
    "- **High** = more randomness and creativity.\n",
    "\n",
    "- **Top-K**\n",
    "- Considers only the **K most likely tokens**.\n",
    "- Low K = more reliable answers.\n",
    "- High K = more creativity.\n",
    "\n",
    "- **Top-P** (nucleus sampling)\n",
    "- Consider tokens until **cumulative probability ‚â§ P**.\n",
    "- **Low P** = less variety.\n",
    "- **High P** = more freedom.\n",
    "\n",
    "**Tip**: Experiment with **temperature + top-k + top-p** to balance creativity and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LLM Output Configuration**\n",
    "\n",
    "**Top-K**, **Top-P** and **Temperature** must be balanced with each other. Each parameter affects the others:\n",
    "\n",
    "- **Temperature = 0** ‚Üí deterministic, *Top-K* and *Top-P* ignored.\n",
    "- **Top-K = 1** ‚Üí only the most probable token is chosen, ignore *temperature* and *top-P*.\n",
    "- **Top-P = 0** ‚Üí effect similar to *top-K = 1*, very restrictive choice.\n",
    "- **High values** for *temperature*, *top-K* or *top-P* ‚Üí more creative, but less controlled output.\n",
    "\n",
    "**Recommended settings**:\n",
    "- Balanced output: `temp = 0.2`, `top-P = 0.95`, `top-K = 30`\n",
    "- Creative: `temp = 0.9`, `top-P = 0.99`, `top-K = 40`\n",
    "- Not very creative / precise: `temp = 0.1`, `top-P = 0.9`, `top-K = 20`\n",
    "- Questions with only one correct answer (e.g. math): `temp = 0`\n",
    "\n",
    "**Beware of the repetition loop**: it can happen both at low temperatures (too rigid), and at high temperatures (too random output). You need to optimize the parameters well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prompting Techniques**\n",
    "\n",
    "**General Prompting / Zero-shot**\n",
    "- Give only instructions or text input **without examples**.\n",
    "- This is the simplest type of prompt, useful for clear and short tasks.\n",
    "- Use low temperatures (e.g. `0.1`) for more specific answers.\n",
    "\n",
    "**One-shot Prompting**\n",
    "- Include **only one example** in the prompt.\n",
    "- It is used to show the format or style of the desired answer.\n",
    "- Useful when the task is not trivial but can be understood with an example.\n",
    "\n",
    "**Few-shot Prompting**\n",
    "- Provide **3‚Äì5 examples or more**, to help the model spot patterns.\n",
    "- Increases the likelihood that the model will follow the desired structure.\n",
    "- Ideal for complex tasks, but be careful about the maximum length of the prompt.\n",
    "\n",
    "**Tips**:\n",
    "- Clear and well-written examples = better results.\n",
    "- Include **edge cases** to increase model robustness.\n",
    "- Track and document versions of your prompts as you design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Types of Prompting (LLM)**\n",
    "\n",
    "#### **1. System Prompting**\n",
    "**Objective:** Defines the **general purpose** of the model and imposes **rules** on the output.\n",
    "\n",
    "**Used to:**\n",
    "- Specify output format (e.g. JSON only, uppercase labels only)\n",
    "- Enforce behaviors (e.g. ‚Äúbe respectful‚Äù)\n",
    "\n",
    "**Example:**\n",
    "> Prompt:\n",
    "> *\"Classify reviews as POSITIVE, NEUTRAL, or NEGATIVE. Return only the uppercase label.\"*\n",
    "> Output: `NEGATIVE`\n",
    "\n",
    "**Another example with JSON:**\n",
    "> Prompt:\n",
    "> *\"Classify reviews and return JSON with specific structure.\"*\n",
    "> Output:\n",
    "> ```json\n",
    "> {\n",
    "> \"movie_reviews\": [\n",
    "> {\n",
    "> \"sentiment\": \"NEGATIVE\",\n",
    "> \"name\": \"Her\"\n",
    "> }\n",
    "> ]\n",
    "> }\n",
    "> ```\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Contextual Prompting**\n",
    "**Objective:** Provides **specific context** for the current prompt, which can be used to generate more relevant responses.\n",
    "\n",
    "**Use to:**\n",
    "- Add background to the prompt\n",
    "- Improve accuracy\n",
    "- Adapt output to a dynamic situation\n",
    "\n",
    "**Example:**\n",
    "> Prompt:\n",
    "> *\"Context: You are writing a blog about arcade games from the 80s. Suggest 3 articles with descriptions.\"*\n",
    "> Output:\n",
    "> - The evolution of arcade cabinets\n",
    "> - Iconic games of the 80s\n",
    "> - The rebirth of pixel art\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Role Prompting**\n",
    "**Objective:** Assign a **role or identity** to the model to influence **tone, style, and knowledge**.\n",
    "\n",
    "**Used to:**\n",
    "- Define a consistent behavior (e.g. ‚Äúyou are a teacher‚Äù, ‚Äúyou are a tour guide‚Äù)\n",
    "- Modulate the tone (formal, humorous, inspirational‚Ä¶)\n",
    "\n",
    "**Standard example:**\n",
    "> Prompt:\n",
    "> *\"Act as a tour guide. I am in Amsterdam and I only want to see museums.\"*\n",
    "> Output:\n",
    "> - Rijksmuseum\n",
    "> - Van Gogh Museum\n",
    "> - Stedelijk Museum\n",
    "\n",
    "**Humorous example:**\n",
    "> Prompt:\n",
    "> *\"You are a comical tour guide. I am in Manhattan.\"*\n",
    "> Output:\n",
    "> - \"Climb to the top of the Empire like King Kong (without a banana)\"\n",
    "> - \"MoMA: art that makes you doubt your talent with stick men\"\n",
    "> - \"Shopping on Fifth Avenue: prepare to cry‚Ä¶ your wallet\"\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "- **System Prompt** = *Rules and structure*\n",
    "- **Contextual Prompt** = *Dynamic and current context*\n",
    "- **Role Prompt** = *Voice, identity, tone of the model*\n",
    "\n",
    "You can combine them to get very powerful and personalized prompts, like:\n",
    "\n",
    "> ‚ÄúAct as an expert teacher (*role*) and explain in a simple way (*system*) how backpropagation works in deep learning, considering that the student is a novice (üîç *context*).‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step-Back Prompting ‚Äì What is it?**\n",
    "It is a technique where you first ask the model a **general question** to activate its ‚Äúbackground knowledge‚Äù, and then use that answer as **context** to answer the real question.\n",
    "\n",
    "Goal: improve accuracy, consistency, and creativity, avoiding generic or stereotyped answers.\n",
    "\n",
    "#### **How ‚Äã‚Äãdoes it work?**\n",
    "\n",
    "1. **Step 1 ‚Äì General question** (e.g. ‚ÄúWhat are 5 iconic settings for an FPS?‚Äù)\n",
    "2. **Step 2 ‚Äì Specific question** (e.g. ‚ÄúWrite a story for a level inspired by one of these settings‚Äù)\n",
    "\n",
    "#### **Example**\n",
    "\n",
    "### Direct prompt:\n",
    "> ‚ÄúWrite the plot of a level of an FPS video game.‚Äù\n",
    "\n",
    "Result: Generic, \"soldiers in the city, shooting, traps, escape...\" style\n",
    "\n",
    "### Step-Back Prompt:\n",
    "\n",
    "#### General Prompt:\n",
    "> ‚ÄúWhat are 5 original settings for an FPS?‚Äù\n",
    "- Abandoned military base\n",
    "- Cyberpunk city\n",
    "- Alien spaceship\n",
    "- Zombie-infested city\n",
    "- Underwater lab\n",
    "\n",
    "#### Final Prompt (with context):\n",
    "> ‚ÄúChoose one of these settings and write the plot of an FPS level.‚Äù\n",
    "\n",
    "Result: Immersive and detailed narrative about an infested underwater lab, with a consistent atmosphere, plot, challenges, and tone.\n",
    "\n",
    "### **Benefits**\n",
    "- Greater activation of latent knowledge\n",
    "- Improved narrative coherence and depth\n",
    "- Reduces bias and stereotypical responses\n",
    "- Extremely useful for creative writing, coding, complex explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What is Chain of Thought (CoT) Prompting?**\n",
    "It is a technique that **improves the reasoning** of LLM models by asking them to **explain the logical steps** before arriving at the final answer.\n",
    "\n",
    "#### Objective:\n",
    "To help the model \"think out loud\", improving the accuracy, interpretability and consistency of the answers.\n",
    "\n",
    "### **Why does it work?**\n",
    "- Activates latent knowledge step by step.\n",
    "- Allows to identify *where* the model goes wrong.\n",
    "- Reduces variations between different models.\n",
    "- Useful for logic, math, code, synthetic data generation problems, etc.\n",
    "\n",
    "### **Simple example**\n",
    "\n",
    "### Direct prompt:\n",
    "> \"When I was 3, my partner was 3 times my age. Now I'm 20. How old is my partner?\"\n",
    "\n",
    "Wrong answer: **63 years old**\n",
    "\n",
    "### With CoT:\n",
    "> \"When I was 3, my partner was 3 times my age. Now I'm 20. **Let's think step by step.**\"\n",
    "\n",
    "Correct answer:\n",
    "1. At 3, the partner was 3√ó3 = 9 years old.\n",
    "2. So difference = 6 years old.\n",
    "3. Now I'm 20 ‚Üí the partner is 20 + 6 = **26 years old**.\n",
    "\n",
    "### **One-shot version (example included):**\n",
    "Give the model **a thoughtful example before your question** to further improve the answer.\n",
    "\n",
    "### **Suggested uses:**\n",
    "- Code (breaking tasks into logical steps)\n",
    "- Mathematics\n",
    "- Creating synthetic data\n",
    "- Generating structured texts (e.g. guided product descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What is Tree of Thoughts (ToT)?**\n",
    "\n",
    "It is an advanced prompting technique that **generalizes** Chain of Thought (CoT), allowing an LLM to:\n",
    "> explore **multiple reasoning paths in parallel**, instead of following just one linear one.\n",
    "\n",
    "#### **How ‚Äã‚Äãdoes it work?**\n",
    "\n",
    "- Each ‚Äúthought‚Äù is a **logical step** or a coherent sequence of text.\n",
    "- Thoughts are **organized in a tree**, where each branch represents a different direction to solve a problem.\n",
    "- The model can **expand multiple nodes**, evaluate the best branches and **choose** the most promising one.\n",
    "\n",
    "#### **Why is it useful?**\n",
    "\n",
    "- Perfect for **complex tasks** where **exploration and multiple evaluation** are needed, such as:\n",
    "- Complex logic problems\n",
    "- Optimization\n",
    "- Advanced coding\n",
    "- Math puzzles\n",
    "\n",
    "#### **Difference from CoT:**\n",
    "\n",
    "| Chain of Thought | Tree of Thoughts |\n",
    "|------------------|------------------|\n",
    "| Single linear path | Multiple simultaneous paths |\n",
    "| Step-by-step | Branch-by-branch |\n",
    "| Less exploration | More creativity, exploration and comparison |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What is ReAct Prompting?**\n",
    "\n",
    "**ReAct** (Reason and Act) is a prompting technique that combines:\n",
    "- **Natural LLM reasoning**\n",
    "- **Action** via external tools (e.g. API, searches, calculation tools)\n",
    "\n",
    "> Inspired by **how a human thinks and acts**: think ‚Üí act ‚Üí observe ‚Üí think again ‚Üí act again ‚Üí until it solves.\n",
    "\n",
    "#### **How ‚Äã‚Äãthe Reason + Act loop works:**\n",
    "\n",
    "1. **Thought:** the model reasons and generates a plan\n",
    "2. **Action:** performs an action (e.g. online search, code, API call)\n",
    "3. **Observation:** observes the result\n",
    "4. **New Thought:** updates the plan with new information\n",
    "5. Continues the loop until it reaches a final answer\n",
    "\n",
    "### **Practical example (LangChain + VertexAI)**\n",
    "\n",
    "**Question:** *How many children do the members of Metallica have?*\n",
    "\n",
    "#### ReAct execution:\n",
    "- LLM searches each member on Google (via SerpAPI)\n",
    "- Reasons: ‚ÄúHetfield has 3 children‚Ä¶ now I look for Ulrich‚Ä¶‚Äù\n",
    "- Progressively adds the children ‚Üí **Final answer: 10**\n",
    "\n",
    "#### **Why use ReAct?**\n",
    "\n",
    "- It allows the LLM to **interact with the world external**\n",
    "- Great for **complex, dynamic or knowledge-based tasks**\n",
    "- First foundations for creating **intelligent autonomous agents** (Agent Modeling)\n",
    "\n",
    "#### **ReAct requires:**\n",
    "- Setup with frameworks like **LangChain**\n",
    "- Access to tools like **SerpAPI**, **code interpreter**, **REST API**\n",
    "- Prompts designed to handle **reasoning ‚Üí action ‚Üí observation loops**\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "ReAct is a powerful paradigm for turning LLMs into **interactive intelligent agents**, capable not only of reasoning, but also **acting in the real world** to get the information they need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Automatic Prompt Engineering (APE)** ‚Äì What is it?\n",
    "\n",
    "APE is a **technique to automate prompt writing**, leveraging **the LLMs themselves to generate, test and optimize prompts**. In practice:\n",
    "> Write a prompt ‚Üí the model generates **variations** ‚Üí evaluate them ‚Üí choose the best one ‚Üí repeat.\n",
    "\n",
    "### **How ‚Äã‚Äãit works (in 3 steps):**\n",
    "\n",
    "1. **Initial prompt (meta-prompt):**\n",
    "You use an LLM (eg: Gemini, GPT, Claude) to generate alternative versions of an existing prompt.\n",
    "\n",
    "**Example:**\n",
    "To train a chatbot on a Metallica t-shirt site:\n",
    "> \"One Metallica t-shirt size S\"\n",
    "Ask the model to generate **10 variations with the same meaning**.\n",
    "\n",
    "2. **Output (generated alternative prompts):**\n",
    "- ‚ÄúI‚Äôd like to purchase a Metallica t-shirt in size small.‚Äù\n",
    "- ‚ÄúCan I order a small-sized Metallica t-shirt?‚Äù\n",
    "- ‚ÄúOne Metallica shirt, size small, please.‚Äù\n",
    "- ...and so on (10 in total)\n",
    "\n",
    "3. **Automatic evaluation:**\n",
    "You compare variants using metrics like:\n",
    "- **BLEU**: measures sentence similarity\n",
    "- **ROUGE**: measures keyword recall\n",
    "\n",
    "Then you select the **best**, or modify it and repeat the process.\n",
    "\n",
    "### **Why is APE useful?**\n",
    "\n",
    "- It **saves you time** in testing prompts manually.\n",
    "- Discover **more effective** prompts for specific tasks (chatbots, classifiers, parsers‚Ä¶).\n",
    "- Automate the more ‚Äúexperimental‚Äù part of prompt engineering.\n",
    "- It is also useful for generating **realistic synthetic data**, useful for NLP or supervised training.\n",
    "\n",
    "### Practical example for prompt generation\n",
    "\n",
    "**Initial APE prompt:**\n",
    "```text\n",
    "We have a band merchandise t-shirt webshop. Generate 10 ways a user could order:\n",
    "\"One Metallica t-shirt size S\". Maintain same meaning, vary phrasing.\n",
    "```\n",
    "\n",
    "**LLM Output:**\n",
    "1. \"I want a Metallica t-shirt, size small.\"\n",
    "2. \"Please order me one small Metallica tee.\"\n",
    "3. \"Can I buy a small Metallica shirt?\"\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Code Prompting ‚Äì What is it?**\n",
    "\n",
    "**Code prompting** is the use of text prompts to:\n",
    "- Write code\n",
    "- Explain code\n",
    "- Translate code\n",
    "- Debug and review code\n",
    "\n",
    "Large Language Models (LLMs) like Gemini, GPT, Claude or PaLM can become real **development collaborators**.\n",
    "\n",
    "### **1. Prompt to write code**\n",
    "\n",
    "**Example**: Rename all files in a folder with `draft_` in front of the name.\n",
    "\n",
    "**Prompt:**\n",
    "> Write a code snippet in Bash, which asks for a folder name. Then it takes the contents of the folder and renames all the files inside by prepending the name `draft`.\n",
    "\n",
    "**Output Bash:**\n",
    "```bash\n",
    "echo \"Enter the folder name: \"\n",
    "read folder_name\n",
    "files=( \"$folder_name\"/* )\n",
    "for file in \"${files[@]}\"; do\n",
    "new_file_name=\"draft_$(basename \"$file\")\"\n",
    "mv \"$file\" \"$new_file_name\"\n",
    "done\n",
    "```\n",
    "### **2. Prompt to explain code**\n",
    "\n",
    "Use the prompt:\n",
    "> \"Explain this Bash code:\"\n",
    "and paste the code.\n",
    "\n",
    "**Expected result**:\n",
    "- Per-line analysis\n",
    "- Clear explanation of input, logic and output\n",
    "\n",
    "### **3. Prompt to translate code (e.g. Bash to Python)**\n",
    "\n",
    "**Prompt:**\n",
    "> Translate this Bash script to Python.\n",
    "\n",
    "**Output (Python):**\n",
    "```python\n",
    "import os, shutil\n",
    "folder_name = input(\"Enter the folder name: \")\n",
    "if not os.path.isdir(folder_name): \n",
    "print(\"Folder does not exist.\") \n",
    "exit(1)\n",
    "for file in os.listdir(folder_name): \n",
    "new_name = f\"draft_{file}\" \n",
    "shutil.move(os.path.join(folder_name, file), os.path.join(folder_name, new_name))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Prompt for debugging**\n",
    "\n",
    "If the code has an error (e.g. `toUpperCase` does not exist), you can paste it and write:\n",
    "\n",
    "> Debug this code. Here‚Äôs the traceback...\n",
    "\n",
    "**Result:**\n",
    "- Explain the problem (e.g. `toUpperCase` does not exist in Python)\n",
    "- Suggest fix (`prefix.upper()`)\n",
    "\n",
    "### **5. General suggestions of the template**\n",
    "\n",
    "The template can also:\n",
    "- Improve code aesthetics (f-strings, consistent names)\n",
    "- Handle errors with `try-except`\n",
    "- Handle files with spaces or special characters\n",
    "- Add logic (e.g. keep file extensions)\n",
    "\n",
    "### Practical applications:\n",
    "\n",
    "- **Automation of systematic tasks** (rename, backup, parsing)\n",
    "- **Explanation of legacy code** (useful for dev teams)\n",
    "- **Collaborative debugging**\n",
    "- **Refactoring**\n",
    "- **Translation between languages** (e.g. Bash ‚Üí Python, Python ‚Üí JS‚Ä¶)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodal Prompting\n",
    "\n",
    "**Multimodal prompting** means using **multiple input formats** other than just text, such as:\n",
    "\n",
    "- **Images**\n",
    "- **Audio**\n",
    "- **Code**\n",
    "- **Video**\n",
    "- **Structured data**\n",
    "\n",
    "**Example**:\n",
    "> \"Describe the content of this image and generate a creative tweet.\"\n",
    "(or)\n",
    "> \"Analyze this audio and generate a transcript + sentiment analysis.\"\n",
    "\n",
    "This type of prompting is only supported by **multimodal models**, such as Gemini Ultra, GPT-4V, Claude 3 Sonnet, PaLM-E, etc.\n",
    "\n",
    "## Best Practices for Prompt Engineering\n",
    "\n",
    "### 1. Provide examples (One-shot, Few-shot)\n",
    "\n",
    "- Examples **teach** the model what to expect and what to generate.\n",
    "- Improve **accuracy, consistency, and style**.\n",
    "\n",
    "**Few-shot Example**:\n",
    "```\n",
    "Input: \"I want a small pizza with mozzarella and tomato sauce.\"\n",
    "Output:\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"toppings\": [\"mozzarella\", \"tomato sauce\"]\n",
    "}\n",
    "```\n",
    "\n",
    "### 2. Use simple and direct language\n",
    "\n",
    "Avoid vague or overly complex sentences.\n",
    "\n",
    "‚ÄúI would like to know interesting facts about this area because I am here on vacation with two 3-year-olds.‚Äù\n",
    "\n",
    "‚ÄúAct as a tour guide. Suggest places to visit in Manhattan with 3-year-olds.‚Äù\n",
    "\n",
    "### 3. Be specific about the desired output\n",
    "\n",
    "Clearly define:\n",
    "\n",
    "- **Format** (e.g. JSON, paragraph, bulleted list)\n",
    "- **Style** (conversational, technical, informal)\n",
    "- **Length** (e.g. ‚Äúwrite in 3 paragraphs‚Äù, ‚Äúin 100 words‚Äù)\n",
    "\n",
    "### 4. Prefer **positive instructions** to **negative constraints**\n",
    "\n",
    "\"Write a 3-paragraph article describing the 5 best consoles. Include name, company, and year.\"\n",
    "\n",
    "\"Write an article but don't include games, don't be too long, don't mention accessories...\"\n",
    "\n",
    "‚Üí **Positive** instructions help the model's creativity and reduce ambiguity.\n",
    "\n",
    "### 5. Control the number of **tokens**\n",
    "\n",
    "- Set a limit (max token) or include it in the prompt:\n",
    "> ‚ÄúWrite it as if it were a tweet.‚Äù\n",
    "> ‚ÄúLimit the answer to 100 words.‚Äù\n",
    "\n",
    "### 6. Use **variables in prompts**\n",
    "\n",
    "For example in an app:\n",
    "```python\n",
    "city = \"Amsterdam\"\n",
    "prompt = f\"You are a travel guide. Tell me a fact about the city: {city}\"\n",
    "```\n",
    "### 7. Experiment with **formats and styles**\n",
    "\n",
    "A prompt can be a:\n",
    "\n",
    "- **Question** ‚Üí \"What made the Sega Dreamcast revolutionary?\"\n",
    "- **Statement** ‚Üí \"The Sega Dreamcast was a sixth-generation console...\"\n",
    "- **Statement** ‚Üí \"Write a paragraph about the Dreamcast and its impact.\"\n",
    "\n",
    "Each produces different results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
